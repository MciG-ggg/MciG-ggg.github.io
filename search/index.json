[{"content":"2025-W44-10 开始学 mit deeplearning 一开始感觉有点吃力， 慢慢看吧 做了 lab1, 感觉 part2 有点难，写个博客总结下 打算走具身智能了 Review Completed Uncompleted Next Week Plan THOUGHTS ","date":"2025-10-29T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/2025-w44-10/","title":"2025-W44-10"},{"content":"2025-W41-10 定一下这学期规划吧，别东搞西搞了。 感觉没有太大必要去看peekaping前端部分，还是走后端吧 可以把dubbogo，seatago多看一点 jyyos的课还有lab做完，老老实实的写还有debug!!! 看完之后，这学期还有空的话课外看看LLM 然后课内就是： 大物课上写作业，看知识点就行了，比较轻松 离散，模电，概统，有点吃力，要多发力 离散，概统看课快的话，把期末卷子写写，然后可以看看CS70 还有逃课最好不要逃课去玩，课上也可以刷课 Review Completed Uncompleted Next Week Plan THOUGHTS ","date":"2025-10-20T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/2025-w41-10/","title":"2025-W41-10"},{"content":"2025-W43-10 这周进社团了，社团招新进了 有工位了，感觉效率高很多 有那种家的感觉了， 然后 jyyos M9 后面换 go 写了，心智负担更低了，leveldb 的文档写挺好的，现在实现一点 memtable 的功能了，简化下实现，大概学一下数据库 Review Completed Uncompleted Next Week Plan 继续写 M9 看 dubbogo, kiwi 从 dubbogo-example 开始看吧，然后学下怎么用 THOUGHTS 有工位学习和生活分开了，回去之后就能好好休息了 然后上周大概规划了每周的每一天做什么，感觉更有掌控感了 ","date":"2025-10-20T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/2025-w43-10/","title":"2025-W43-10"},{"content":"2025-04/21~27 Review 折腾的一周： 折腾 obsidian 折腾个人博客 CS188 就看了一节 感觉得慢慢看，不要急 所以复习了下前面的内容，打算写篇博客文章，还没写完 proj, 和 discussion 的进度也要跟上 五一结束前估计搞不完了，看看 5 月中旬能不能搞完 写了点小工具，不过是重复造轮子罢了,,, 重启 leetcode, 好久没刷了，每天花一点点时间刷刷 Completed Uncompleted CS188proj2, proj3 还有前面的 discussion Next Week Plan 等待五一爽学爽玩 每天抽时间看书，刷 leetcode CS188proj2 🔺 ➕ 2025-04-27 📅 2025-04-30 ✅ 2025-04-29 CS188proj3 🔺 ➕ 2025-04-29 ⏳ 2025-05-02 📅 2025-05-02 ✅ 2025-05-0a1 CS70 概率论部分 note ⏫ ➕ 2025-04-29 📅 2025-05-11 CS188discussion 🔼 ➕ 2025-04-29 ✅ 2025-06-09 线代期末复习 🔺 📅 2025-05-05 ✅ 2025-05-08 电路作业 ⏫ ⏳ 2025-05-05 ✅ 2025-05-08 电路大物实验报告 🔼 ⏳ 2025-05-02 ✅ 2025-05-03 THOUGHTS 折腾的最终境界就是默认配置\u0026hellip; ","date":"2025-10-08T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/2025-04/21~27/","title":"2025-04/21~27"},{"content":"2025-W24-06 Review 这周做了一堆实验, 实验报告要写死了 课外的事情没做啥, 到处折腾了一点 Completed 看了点 cs144 的proj, 感觉有点意思 开始学 go, 语法基本差不多, 主要是多一个 go routine 和channel Uncompleted Next Week Plan 把 go 基本语法看完 做个小项目 THOUGHTS 感觉自己进入信息爆炸了, 天天闲着就看手机, 手机拿在手上不松的. ","date":"2025-10-08T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/2025-w24-06/","title":"2025-W24-06"},{"content":" Title 好的协议不是防止所有错误，而是确保错误能被快速检测和修复 大概就是从头实现路由器算法的每个规则.\n每个需要实现的函数, 都会以事件驱动的方式, 被框架的模拟器调用. 所以我们只需要实现具体函数即可, 调用过程不用管.\nstage1: 安装静态路由 先实现安装静态路由.\n由\u0026quot;一个节点第一次连接到一个路由器\u0026quot; 事件触发 比较简单, 没有什么太多说的. 大概讲一下 TableEntry: classDiagram class TableEntry { +dst : 目的地址 +port : 下一跳端口 +latency : 路径代价/延迟 +expire_time : 过期时间 } Title 这里的 latency 就相当于课本里所说的 cost TableEntry 只告诉你我这个路由器能到哪里, 以及下一步你该去哪里. stage2: 转发数据包 用 packet 的 dst ,从路由表中找到对应的路由表项 检查路由表项是否为空 or 路由表项. latency \u0026gt;= INFINITY 如果为 false: 说明这个 packet 可以转发 (有下一个目的地, 且下一个目的地可以到达 (latency \u0026lt; INFINITY)) stage3: 转发路由表 对某一个路由器,定时转发其所有路由表项, 向其所有邻居 Title 要向该路由器所有邻居转发其所有路由表项 从路由表中查找到各个邻居需要走的端口, 从那些端口转发自己所有的路由表项. stage4: 更新路由表 当路由器从邻居收到一条到某个目的地（route_dst）的路由通告时，决定是否更新自己的路由表。 主要流程 计算总延迟\ntotal_latency = route_latency + self.ports.get_latency(port)\n总延迟 = 邻居到目的地的延迟 + 本地到邻居的延迟\n设置过期时间\nexpire_time = api.current_time() + self.ROUTE_TTL\n路由项的有效期\n路由表更新规则\n如果路由表中没有该目的地（route_dst），直接添加新项。 如果已有该目的地： 如果原来的下一跳端口就是这次收到通告的端口，直接更新（刷新延迟和过期时间）。 如果新通告的总延迟更小，则更新为更优的路径。 否则，不做任何更改。 stage5: 处理超时 定期检查路由表，把已经过期的路由项删除，保证路由表的有效性和准确性。 主要流程 遍历路由表所有项\nfor _, table_entry in list(t.items()):\n遍历当前路由表中的每一项。\n判断是否过期\nif table_entry.expire_time != FOREVER and table_entry.expire_time \u0026lt; api.current_time():\n如果该路由项的过期时间不是永久（FOREVER），并且已经小于当前时间（即已经过期），则进入下一步。 日志记录\nself.log(f\u0026quot;Route expired: dst={table_entry.dst}, port={table_entry.port}, latency={table_entry.latency}\u0026quot;)\n记录一条日志，说明哪一项已经过期。 删除过期项\nt.pop(table_entry.dst)\n从路由表中删除这条过期的路由项。 基本的路由器功能已经实现, 下面是一些优化和问题修复\nstage6: 水平分割 水平分割: 在路由器互相发通告, 转发路由表的时候,不向学到该路由的端口通告该路由, 防止两个路由器无限循环互相转发数据 (也仅能防止两个路由器, 三个及以上做不到) stage7: Poison Reverse Poison Reverse: 如果我的到达某目的地的最短路径是通过你，那么我在通告你的时候，告诉你我到该目的地的距离是无穷大 stage8: Counting to Infinity 以上两种方式都无法避免:\n假设A、B、C形成环，A到C的链路断开： A发现到C不可达，距离变大。 B还没意识到，告诉A“我到C距离是2”，A更新为3。 下一轮，B又从A学到“3”，B更新为4…… 核心思想：只要距离超过INFINITY，通告时就直接用INFINITY，防止路由器之间互相“抬高”距离，网络能快速收敛。 把大于 INFINITY 的距离直接设为 INFINITY，核心目的就是为了防止路由器之间继续互相“抬高”数值 stage9: Poisoning Expired Routes 当路由过期时，不是直接删除它，而是将其标记为无限大（毒化），并继续传播这个\u0026rsquo;死亡通知\u0026rsquo;一段时间 直接删除的核心问题：信息黑洞效应 当路由器直接删除失效路由时，会立即形成信息真空： 下游路由器不知情：相邻路由器仍需等待自己的超时计时器到期 临时环路风险：其他节点可能继续向已删除路由的节点发送流量 典型案例（7节点拓扑）： h1 \u0026ndash; s1 \u0026ndash; s2 \u0026ndash; s3 \u0026ndash; s4 \u0026ndash; s5 \u0026ndash; s6 \u0026ndash; s7 当s1-s2断开： 若s2直接删除h1路由： s3仍认为可通过s2到达h1（跳数=3） s3继续向s2转发数据包 → 黑洞丢包 title 删除是简单的，但正确的删除需要智慧 stage10: 触发更新 1. 历史记录结构（self.history） 在__init__中添加了self.history = {}。 结构为： {port: {dst: latency}} (一个嵌套字典) 作用：记录“上次通过每个端口(port)通告给每个目的地(dst)的延迟(latency)”。 2. 辅助函数 should_advertise(self, port, dst, latency) 判断是否需要通告。 如果历史中没有该端口或目的地，说明是第一次通告，返回True。 如果历史中有，但延迟发生了变化，也返回True。 否则返回False（即内容没变，不需要通告）。 update_history(self, port, dst, latency) 每次通告后，更新历史记录。 保证下次判断时有最新的历史数据。 3. send_routes 遍历所有端口和路由表项。 对于每个端口-目的地组合，先判断是否需要通告（force为True时无条件通告）。 只有需要通告时才真正发送，并更新历史。 支持single_port参数，只向指定端口通告（用于链路刚上线时）。 4. handle_route_advertisement 收到邻居的路由通告时，只有当本地路由表真的被更新（新增、更优、同端口刷新）时，才调用self.send_routes(force=False)，即只在有变化时触发增量通告。 5. handle_link_up 当有新链路上线时，如果SEND_ON_LINK_UP为True，立即用send_routes(force=True, single_port=port)向新邻居通告所有路由。 6. handle_link_down 当链路断开时： 如果POISON_ON_LINK_DOWN为True，把所有走该端口的路由毒化（latency=INFINITY），并用send_routes(force=False)增量通告毒化结果。 否则，直接删除所有走该端口的路由。 7. 增量通告的核心 只通告“有变化”的路由（即历史记录和当前要通告的内容不同）。 这样可以减少冗余消息，提高网络效率。 ","date":"2025-10-08T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/cs168proj2/","title":"cs168proj2"},{"content":" CS188：从搜索问题到 RL——人工智能决策算法的演进之路 UC Berkeley 的 CS188 课程（人工智能导论）以“构建智能体的决策能力”为核心，从经典的搜索算法到现代强化学习（Reinforcement Learning, RL），系统性地揭示了人工智能如何在复杂环境中规划路径、解决问题并优化策略。本文将沿着课程脉络，梳理从基础搜索到强化学习的演进逻辑，并探讨其背后的核心思想。\n第一部分：确定性世界的决策基础——搜索问题 1. 搜索问题的数学定义 搜索问题的目标是找到从初始状态到目标状态的最优路径（或满足条件的可行路径）。其核心在于形式化环境的结构，使智能体能够系统化地探索可能的行动序列。\n2. 搜索问题的构成要素 (1) 状态空间（State Space） 定义：所有可能状态的集合，描述智能体在环境中的可能配置。 示例： 迷宫问题：状态是坐标 (x, y)。 拼图游戏：状态是棋盘的布局（如八数码问题）。 机器人导航：状态是机器人的位置与朝向。 (2) 动作集合（Actions） 定义：在某个状态下可执行的操作集合，记为 Actions(s)。 示例： 迷宫中：{上, 下, 左, 右}。 拼图游戏：{空格与相邻数字交换}。 自动驾驶：{加速, 刹车, 左转, 右转}。 (3) 转移模型（Transition Model） 定义：函数 Result(s, a) → s'，描述在状态 s 执行动作 a 后到达的下一状态 s'。 示例： 迷宫中：Result((x,y), 上) → (x, y+1)（假设坐标系向上为y轴正方向）。 拼图游戏：Result(当前布局, 左移空格) → 新布局。 (4) 动作成本（Action Cost） 定义：从状态 s 通过动作 a 转移到 s' 的即时成本，记为 Cost(s, a, s')。 作用：用于衡量路径的优劣（如最短路径、最低能耗）。 示例： 迷宫中：每步成本为1（路径长度即总成本）。 导航问题：成本为道路长度或行驶时间。 (5) 初始状态（Start State） 定义：智能体开始执行任务时的初始配置。 示例： 罗马尼亚度假问题：初始状态是城市 Arad。 拼图游戏：初始状态是打乱的棋盘布局。 (6) 目标测试（Goal Test） 定义：函数 GoalTest(s) → True/False，判断状态 s 是否为目标状态。 示例： 迷宫问题：GoalTest((x,y)) → (x,y)是否为终点坐标。 拼图游戏：GoalTest(s) → s是否为目标布局。 3. 搜索问题的求解目标 智能体需要找到从初始状态到目标状态的最优路径（或可行路径），其中“最优”由路径总成本定义。例如：\n最短路径：动作成本为1时，BFS可找到最优解。 最低成本路径：动作成本不同时，需使用一致代价搜索（UCS）。 4. 示例：罗马尼亚度假问题 状态空间：罗马尼亚城市（如Arad, Bucharest, Sibiu等）。 动作：从当前城市可直达的相邻城市（如从Arad可到Sibiu、Timisoara等）。 转移模型：Result(Arad, 前往Sibiu) → Sibiu。 动作成本：城市间的道路长度（如Arad到Sibiu为140公里）。 初始状态：Arad。 目标测试：当前城市是否为Bucharest。 求解目标：找到从Arad到Bucharest的最短路径（如Arad → Sibiu → Rimnicu Vilcea → Pitesti → Bucharest）。\n5. 搜索问题与智能体的关系 理性决策：智能体需在有限计算资源下，选择能最大化未来累积奖励（或最小化成本）的行动。 搜索算法的角色：通过系统化地探索状态空间（如BFS、A*、UCS等），找到满足目标的最优路径。 6. 关键挑战 状态空间爆炸：状态数量可能随问题规模指数增长（如棋盘游戏）。 最优性与效率的权衡：如何在合理时间内找到最优解（如启发式搜索A*）。 动态环境：若环境变化（如交通拥堵），需重新规划路径（需结合在线搜索）。 总结 搜索问题的形式化定义为智能体提供了在复杂环境中规划行动的数学框架。通过明确状态空间、动作、转移模型等要素，结合适当的搜索算法（如BFS、A*、UCS），智能体能够高效地找到从初始状态到目标状态的最优路径。这一过程是构建理性规划智能体的基础，也是后续学习更高级算法（如强化学习）的前提。\n搜索问题的解法 把搜索问题抽象成图论问题 把每个状态看成一个节点 把每个action 看成节点之间的有向边 用图的搜索算法来解决搜索问题 第二部分：不确定性与动态环境——从 MDP 到强化学习 1. 马尔可夫决策过程（MDP） MDP 是建模随机环境的数学框架，包含：\n状态 $s$ 、动作 $a$ 、转移概率 $T(s,a,s')$：动作可能导致多个后续状态。\n**奖励函数 $R(s,a,s')$ **：定义每个状态的即时收益。\n**折扣因子 $\\gamma$ **：权衡当前与未来奖励。\n求解方法：\n值迭代：通过贝尔曼方程迭代更新值函数，收敛到最优策略。\n公式： $$V_{k+1}(s) = \\max_{a}\\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V_{k}(s')]$$ 用 $s$ 的下一个状态 $s'$ 的第 $k$ 次迭代的V 值 $V_{k}(s')$ 来更新第 $k+1$ 迭代的状态 $s$ 的V 值 $V_{k+1}(s)$ 策略迭代：交替执行策略评估与改进，直接优化策略。 两个步骤：\n策略评价 目标：计算当前策略 $\\pi$ 的状态值函数 $V^\\pi(s)$ 。 方法：通过迭代求解贝尔曼期望方程： $$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]$$ 其中： $R(s,a)$ 是即时奖励， $P(s'|s,a)$ 是状态转移概率， $\\gamma$ 是折扣因子。 终止条件：当值函数的变化小于阈值 $\\theta$（如 $10^{-3}$）时停止迭代。 策略改进/策略提取 目标：基于当前值函数 $V^\\pi(s)$ 生成更优策略 $\\pi'$ 。 方法：对每个状态 $s$，选择使动作值函数 $Q^\\pi(s,a)$ 最大的动作： $$\\pi'(s) = \\arg\\max_{a} \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s') \\right]$$ 若新策略 $\\pi'$ 与原策略 $\\pi$ 相同，则停止迭代；否则，用 $\\pi'$ 替代 $\\pi$ ，重复策略评估。 2. 强化学习（RL）：无模型的动态决策 RL 问题：仍然是MDP 模型，但是不知道 $T(s,a,s')$ 或 $R(s,a,s')$ 两个主要问题： 怎么从尝试过的动作中学习（被动RL） 两种解决办法： 基于模型的RL：尝试获取 $T(s,a,s')$ 或者 $R(s,a,s')$ ，从而变回MDP 问题，用策略迭代或者值迭代即可 不基于模型的RL：不获取 $T(s,a,s')$ 或者 $R(s,a,s')$ ，直接通过 $Q$ 值来学习策略 $\\pi$ 怎么基于学到的东西去选择执行的动作（在探索（尝试新动作以发现更好策略）和利用（选择当前最优动作）之间权衡）（主动RL）： $\\epsilon$ -贪心： $\\epsilon$ 的概率随机探索（exploration）， $1-\\epsilon$ 的概率采取利用（exploitation）当前数据得出的最佳值 exploration function 等等 被动RL 基于模型的RL和不基于模型的RL 的区别： Title 左侧：基于数据去获取P（a），可以理解为对世界建模，理解概率的分布 右侧：基于采样的数据直接获取A 的期望，并不知道概率分布（对世界的具体模型没有了解） 不基于模型的RL：当环境模型未知时，强化学习通过试错直接学习策略。核心算法 Q 学习通过时序差分更新逐步逼近最优动作价值函数：\n$$ Q (s, a) \\leftarrow Q (s, a) + \\alpha \\left[ R (s, a) + \\gamma \\max_{a'} Q (s', a') - Q (s, a) \\right] $$ Q-learning 是离策略 (Off-policy) 离策略算法的核心是将目标策略（Target Policy）的学习与行为策略（Behavior Policy）的数据收集过程分离。 目标策略（π） ：希望学习的最优策略，通常通过最大化期望回报得到（如 Q-learning 中的贪婪策略）。 行为策略（μ） ：用于与环境交互、生成训练数据的策略，通常包含探索机制（如ε-greedy、Boltzmann 探索）。 特性 离策略（Off-Policy） 在线策略（On-Policy） 策略关系 目标策略 ≠ 行为策略 目标策略 = 行为策略 数据来源 可复用历史数据或外部数据 必须实时与环境交互生成新数据 典型算法 Q-learning、DQN、DDPG、SAC SARSA、REINFORCE、PPO 数据效率 高（数据可重复使用） 低（需持续交互） 实现复杂度 高（需处理分布偏移） 低（策略一致性） 解释： 为什么学习 $Q$ 值而不是 $V$ 值： 如果使用 V 值，智能体需要知道环境的动态模型（即状态转移概率 $T(s,a,s')$ 和奖励函数 $R(s,a,s')$ ），才能通过贝尔曼方程计算动作的期望价值. 而通过直接学习 Q(s,a)，绕过了对环境模型的依赖 Q值的灵活性与V值的不足 ：Q(s,a) 显式地为每个动作分配价值，允许智能体在探索（如随机动作）和利用（选择当前最优动作）之间灵活切换。若使用 V 值，智能体需要额外机制（如策略函数）将状态价值映射到动作选择，这会增加复杂度。 Q 学习 vs. MDP：\nMDP 需已知环境模型（转移概率与奖励函数），RL 无需模型。 Q 学习通过经验（状态-动作-奖励序列）直接学习，适用于动态环境。 主动RL $\\epsilon$ - greedy exploration function 把通过某个 action $a'$ 到达某个状态 $s'$ 的次数 $N(s',a')$ 纳入考虑，用 $f(Q(s',a'), N(s',a'))$ 取代原先的 $Q(s',a')$ $$Q(s,a) \\leftarrow (1-\\alpha)Q(s,a) + \\alpha[R(s,a,s') + \\gamma \\max_{a'}f(Q(s',a'), N(s',a'))]$$ Approximate Q-learning 介绍 Approximate Q-learning 是经典 Q-learning 算法的扩展，旨在解决传统 Q-learning 在高维或连续状态空间中的维度灾难问题。其核心思想是通过函数逼近（Function Approximation）代替传统的表格（Tabular）存储，利用参数化的函数（如线性模型、神经网络）近似表示 Q 值，从而实现对大规模状态空间的泛化能力。\n1. 为什么需要 Approximate Q-learning？ 传统 Q-learning 的局限性\n在表格型 Q-learning 中，Q 值存储为一个二维表 $Q(s, a)$，每个状态-动作对需要单独维护一个值。当状态空间巨大（如围棋的 $10^{170}$ 种状态）或连续（如机器人传感器数据）时，表格存储和更新变得不可行。\n函数逼近的优势\n使用参数化函数 $Q(s, a; \\theta)$（如线性模型、神经网络）代替表格，通过调整参数 $\\theta$ 逼近真实 Q 值。这种方法可以：\n泛化：相似状态共享参数，避免重复学习。 处理连续状态：直接接受实数向量作为输入（如传感器数据）。 降低存储成本：参数规模远小于状态-动作组合数。 2. 算法原理 Approximate Q-learning 的更新规则基于传统 Q-learning，但引入函数逼近和梯度下降优化：\n目标 Q 值计算\n与传统 Q-learning 类似，目标 Q 值为： $$\\text{Target} = r + \\gamma \\max_{a'} Q(s', a'; \\theta)$$ 其中 $s'$ 是下一状态，$\\gamma$ 是折扣因子。\n损失函数与参数更新\n定义损失函数为预测 Q 值与目标 Q 值的均方误差： $$L(\\theta) = \\frac{1}{2} \\left[ Q(s, a; \\theta) - \\text{Target} \\right]^2$$ 通过梯度下降更新参数 $\\theta$： $$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta)$$ 其中 $\\alpha$ 是学习率，梯度方向由链式法则计算： $$\\nabla_\\theta L(\\theta) = \\left( Q(s, a; \\theta) - \\text{Target} \\right) \\nabla_\\theta Q(s, a; \\theta)$$ 伪代码示例\n1 2 3 4 5 6 7 8 9 for each episode: initialize state s while not terminal: choose action a via ε-greedy(Q(s, a; θ)) execute a, observe reward r and next state s\u0026#39; target = r + γ * max_a\u0026#39; Q(s\u0026#39;, a\u0026#39;; θ) loss = 0.5 * (Q(s, a; θ) - target)^2 θ = θ - α * (Q(s, a; θ) - target) * ∇θ Q(s, a; θ) s = s\u0026#39; 3. 函数逼近方法 线性函数逼近\n用线性组合表示 Q 值：\n$$Q(s, a; \\theta) = \\theta^T \\phi(s, a)$$\n其中 $\\phi(s, a)$ 是状态-动作的特征向量（如人工设计的特征或自动编码的特征）。\n非线性函数逼近（如神经网络）\n使用深度神经网络（DQN）自动提取状态特征：\n$$Q(s, a; \\theta) = \\text{NeuralNetwork}(s, a; \\theta)$$\n深度 Q 网络（DQN）通过经验回放（Experience Replay）和目标网络（Target Network）稳定训练。\n4. 优缺点分析 优点 缺点 可处理高维/连续状态空间 收敛性不保证（函数逼近可能发散） 泛化能力强，避免维度灾难 需谨慎设计特征或网络结构 适用于真实场景（如图像、传感器输入） 超参数（学习率、网络架构）敏感 5. 应用场景 游戏 AI：如 Atari 游戏（DQN 直接以像素为输入） 机器人控制：连续传感器数据映射到动作（如机械臂抓取） 资源调度：大规模状态空间下的动态决策（如云计算资源分配） 6. 扩展与改进 深度 Q 网络（DQN）：结合神经网络与经验回放、目标网络（见 Nature DQN）。 Double Q-learning：减少最大化偏差（Maximization Bias）。 Dueling DQN：分离状态价值 $V(s)$ 和动作优势 $A(s, a)$，提升策略评估效率。 1. 策略搜索（Policy Search） 当基于特征的策略难以通过 Q-learning 直接优化时，策略搜索通过直接优化策略参数（而非 Q 值）提升性能。\n核心思想\nQ-learning 的局限：Q-learning 优先准确估计 Q 值（建模），但特征设计的偏差可能导致策略次优。 策略搜索的目标：直接优化策略参数，最大化累积奖励（预测），而非精确拟合 Q 值。 方法分类\n简单策略搜索：通过扰动特征权重（如线性 Q 函数的权重），评估策略改进方向（需大量采样）。 策略梯度（Policy Gradient）：基于蒙特卡洛采样，通过梯度上升更新策略参数，使高奖励轨迹的动作概率增加。 近端策略优化（PPO）：改进策略梯度，限制策略更新的幅度，提升训练稳定性。 应用场景\n连续动作空间（如机器人关节控制）：Q-learning 需计算 $\\max_a Q(s,a)$，而连续动作难以枚举，策略搜索直接输出动作分布。 高维/复杂策略（如语言生成）：直接优化生成策略（如选择下一个词的概率分布）。 2. 案例分析（Case Studies） 强化学习在多个领域展现强大能力，以下是三个典型应用：\n案例 1：Atari 游戏（Atari Game Playing） MDP 设置 状态：游戏屏幕的像素（$84 \\times 84$ 图像）。 动作：手柄按键组合（如 18 种离散动作）。 挑战：状态空间巨大（$256^{84 \\times 84}$），无法使用表格法。 解决方案 深度 Q 网络（DQN）：用卷积神经网络逼近 Q 值，结合经验回放和目标网络稳定训练。 探索策略：ε-greedy 平衡探索与利用。 案例 2：机器人运动控制（Robot Locomotion） MDP 设置 状态：机器人传感器数据（关节角度、加速度等连续向量）。 动作：电机指令（连续向量）。 挑战：真实世界训练成本高、风险大。 解决方案 仿真训练迁移：先在模拟器中训练策略，再部署到真实机器人。 策略搜索方法：PPO 优化连续动作策略，最大化前进速度、保持平衡等奖励。 案例 3：语言助手（Language Assistants） MDP 设置 状态：已生成的文本序列（如“What is population of Berkeley?”）。 动作：选择下一个词（词汇表大小约 10 万）。 奖励：人工标注或奖励模型（如回答准确性、流畅性）。 解决方案 策略梯度：直接优化生成策略，调整词的概率分布。 挑战：动作空间极大（10 万词），需高效采样与梯度估计。 3. 后续方向与总结 课程后续内容：转向 不确定性（Uncertainty） 与 学习理论（Learning），涵盖贝叶斯网络、隐马尔可夫模型等。 强化学习的核心挑战： 探索与利用的平衡：如何在复杂环境中高效探索（如基于好奇心的内在奖励）。 样本效率：减少真实环境中的交互次数（如元学习、模仿学习）。 安全与鲁棒性：确保策略在真实场景中的可靠性（如安全约束强化学习）。 ","date":"2025-10-08T00:00:00Z","image":"https://mcig-ggg.github.io/p/cs188%E4%BB%8E%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E5%88%B0rl/Pasted-image-20250424203806_hu_c8ca0efc69b3720e.png","permalink":"https://mcig-ggg.github.io/p/cs188%E4%BB%8E%E6%90%9C%E7%B4%A2%E9%97%AE%E9%A2%98%E5%88%B0rl/","title":"CS188：从搜索问题到RL"},{"content":"docker底层原理简介 #conclusion\ndocker 为了实现容器间的隔离，资源的限制，高效性，分别使用了 linux namespace, Cgroups(control groups), Union FS 这几个技术，下面一一介绍\nLinux namespace 刚开始见到会联想到 cpp 的 namespace,了解之后发现二者在效果上差不多都是： 隔离某些东西（linux namespace：pid 对应的进程，一个pid在不同的namespace对应不同进程， cpp namespace： 一个函数名在不同namespace对应不同函数）\n具体原理 以前：操作系统里 pid 和进程一一对应，不可能有进程的pid相同 现在：操作系统增加一个 osid ，现在一个进程的识别要同时匹配 osid， pid，一个进程树对应一个osid， 大概如下 Note 一个进程树对应一个osid 现在进程树隔离了,这样我们就可以创建两个不干扰的容器了 Cgroups 前面可以让两个容器隔离了，现在还需要控制容器的 cpu 占用，内存占用等等，不能让容器随意使用系统资源。Cgroups 通过层级结构（hierarchy）和控制组（group）来管理资源。每个层级可以包含多个控制组，每个控制组可以包含多个进程。每个控制组可以独立地配置资源限制和优先级。\nCgroups的数据结构存储在 /sys/fs/group 下，大概有这些目录结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 /sys/fs/cgroup/ ├── cgroup.controllers ├── cgroup.max.depth ├── cgroup.max.descendants ├── cgroup.pressure ├── cgroup.procs ├── cgroup.stat ├── cgroup.subtree_control ├── cgroup.threads ├── cpu.pressure ├── cpuset.cpus.effective ├── cpuset.cpus.isolated ├── cpuset.mems.effective ├── cpu.stat ├── cpu.stat.local ├── dev-hugepages.mount ├── dev-mqueue.mount ├── dmem.capacity ├── dmem.current ├── init.scope ├── io.cost.model ├── io.cost.qos ├── io.pressure ├── io.prio.class ├── io.stat ├── irq.pressure ├── machine.slice ├── memory.numa_stat ├── memory.pressure ├── memory.reclaim ├── memory.stat ├── memory.zswap.writeback ├── misc.capacity ├── misc.current ├── misc.peak ├── proc-sys-fs-binfmt_misc.mount ├── sys-fs-fuse-connections.mount ├── sys-kernel-config.mount ├── sys-kernel-debug.mount ├── sys-kernel-tracing.mount ├── system.slice └── user.slice 然后在 /proc/*/cgroup 下会有各个进程的所属的控制组信息，每一行表示一个进程在某个层级中的控制组路径。格式如下： \u0026lt;层级号\u0026gt;:\u0026lt;子系统\u0026gt;:\u0026lt;控制组路径\u0026gt;\n层级号：表示控制组所属的层级编号。 子系统：表示控制组管理的资源类型，例如 cpu、memory、blkio 等。 控制组路径：表示进程所属的控制组路径，从根控制组开始。 举个例子：\n0::/user.slice/user-1000.slice/ user@1000.service /app.slice/ app-Hyprland-dunst@3883b287.service ：表示进程属于用户 1000 的 Hyprland-dunst 服务。路径中的每一级都表示一个控制组的层级结构。\nuser.slice：表示用户级别的控制组。 user-1000.slice：表示用户 1000 的控制组。 user@1000.service：表示用户 1000 的服务。 app.slice：表示应用程序级别的控制组。 app-Hyprland-dunst@3883b287.service：表示 Hyprland-dunst 服务的具体实例。 还有： 0::/system.slice/systemd-userdbd.service\nsystem.slice：表示这是一个系统级别的控制组。 systemd-userdbd.service：表示这是 systemd-userdbd 服务。 Union FS UnionFS是一种联合文件系统技术，它允许多个文件系统挂载到同一目录下，合并成一个整体视图。它采用分层结构，底层为只读层，顶层为可写层，数据修改只发生在顶层，利用“写时复制”机制确保底层数据不受影响。UnionFS在容器技术中广泛应用，如Docker使用类似的OverlayFS管理镜像分层。它优势在于高效存储、快速更新和简化管理，但也存在文件操作复杂性和潜在一致性问题。\ngraph TD A[\"Merged View: file1.txt (modified)\"] --\u003e B[\"Writable Work Layer\"] A --\u003e C[\"Read-Only Lower Layer\"] B --\u003e D[\"file1.txt (modified)\"] C --\u003e E[\"file1.txt (original)\"] style A fill:#ffe6ff,stroke:#333 style B fill:#ffe6e6,stroke:#ff6666 style C fill:#e6e6ff,stroke:#6666ff style D fill:#ffe6e6,stroke:#ff6666 style E fill:#e6e6ff,stroke:#6666ff ","date":"2025-10-08T00:00:00Z","image":"https://mcig-ggg.github.io/p/docker%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/Pasted-image-20251006220327_hu_17c588da0719f918.png","permalink":"https://mcig-ggg.github.io/p/docker%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/","title":"docker底层原理简介"},{"content":" 少折腾 记录下现在的成果 主页 时间线 添加时间节点：在 Home 里把时间线的 events 加一项 习惯记录 更改标题：更改 dv.span(\u0026quot;** 📖 阅读 **\u0026quot;) 括号中的内容 更改习惯分类：更改 const habitIntensity = getHabitInPage(page, 'reading') 中‘reading’为对应的习惯分类，如 outputing 更改颜色：更改 color: \u0026quot;orange\u0026quot; 中为 calendarData 中对应颜色 记录每天的习惯：如- [x] [[obsidian折腾总结]] #habit outputing:: 1 ✅ 2025-04-22 注意格式： #habit 必须在 outputing 等前面 outputing 后面要紧跟双冒号 :: 冒号后面接的是颜色深度，有范围限制 contribution 图 tasks tasks canlendar wraper ","date":"2025-10-08T00:00:00Z","image":"https://mcig-ggg.github.io/p/obsidian%E6%8A%98%E8%85%BE%E6%80%BB%E7%BB%93/Pasted-image-20250422233237_hu_a4bbbd6e0525159a.png","permalink":"https://mcig-ggg.github.io/p/obsidian%E6%8A%98%E8%85%BE%E6%80%BB%E7%BB%93/","title":"obsidian折腾总结"},{"content":"大一下学期总结 这学期的心态变了很多,我也不想在这篇文章里写太多技术相关的东西.主要就是记录下我目前的一些想法\n学习 一开学想这学期好好学OS, 然后刚开学在看rcore,想这学期把rcore搞完,但是后面又没看了,感觉没啥意义. 后面看了几集jyyos,也没看了 后面就被忽悠去搞大创了,一个月边写申报书边看论文,了解了一点SLAM相关, 不过最后申报书没过,幸好我也不太想走这方面. 再后面看了点CS188, 了解了一点RL相关,后面有讲AI其他方面的概述,不知道暑假还继不继续看 这学期课内事情太多了,有点应付不过来 暑假打算看看计网,6.824 . 一些想法 我感觉我在人际交往还有个人还有别的方面有了很多变化\n遇到了一些人,后面进了我们学校的\u0026quot;地下开源协会\u0026quot;,全是大佬,有大二进wxg前端实习的,有同时是archlinuxcn,nix,deepin维护者的大佬.感觉稍微了解了一点开源这个世界\n打了学校的排球阳光杯,虽然没上场.看大四老登在上面拼命打,拿下最后一球,赢了季军赛之后,坐在地板上哭,挺感慨的.想起了高三在我们的小高中的运动会,最后一跳三级跳拿下第一的事情(虽然厉害的人都去报跳远了). 时间总是这样流过.\n这学期看了星际牛仔,漂流少年,冰海战记,怪奇物语.(阿谢拉特很帅) 某人能让我从别人的角度,另外的角度看自己看世界,我很感谢\n爱到底是什么呢,该怎么去爱人呢,我很讨厌教育体系内缺少这方面的教育,明明是人生中重要的东西,但是大部分人却对此不甚了解,只能自己慢慢摸索.人生不应该只有面前的绩点,未来的保研,追求的工作,拿到的工资.人生中很多东西都很重要,所以请不要忘记自己心里重要的东西,自己身边重要的东西.\n这学期接触了一些学校里的人之后,愈发感觉大学内部的信息流通实在是太闭塞,比如说我们学校里的开源协会,我原本以为学校里没有什么技术氛围,没想到还是有的(不过里面基本都是老登)\n每个人都有一个不大不小的梦\n这学期后面开始偶尔喝点酒,以前挺抗拒喝酒的,不过现在喝的也基本只是鸡尾酒.前段时间开源协会有人说要搞聚会,虽然最后也就几个人,吃晚饭去酒吧聊天.第一次去酒吧,感觉遇到的人经历的事听到的东西对我的世界观冲击有点大\n人生还有很长啊.\n","date":"2025-10-08T00:00:00Z","image":"https://mcig-ggg.github.io/p/%E5%A4%A7%E4%B8%80%E4%B8%8B%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/73179512905131d196dea717dd735482_720_hu_73ec37abf4b10606.jpg","permalink":"https://mcig-ggg.github.io/p/%E5%A4%A7%E4%B8%80%E4%B8%8B%E5%AD%A6%E6%9C%9F%E6%80%BB%E7%BB%93/","title":"大一下学期总结"},{"content":"起因 事情的起因是复制 ai 回答的内容里的 LaTeX 公式，它所采用的是 \\( \\) (行内公式) 和 \\[ \\] （行间公式），而 obsidian 默认支持的是 $ $ 和 $$ $$ ，然后复制过来的话要对内容进行修改才能成功渲染LaTeX 公式 手动修改无疑是无意义的重复，但是我一直懒得找解决方法，直到今天实在受不了了。 经历 最开始的想法：看看能不能更改 obsidian 支持的 latex 格式，但是我没找到（也许有，可能只是我没找到） 后来的想法：因为对这种字符串操作，第一反应就是 vim, 然后我记得 obsidian 是有 vim 模式的，于是我开了 obsidian 的 vim 模式，尝试手动用 vim 命令更改粘贴后的内容,,, 但是好像 obsidian 的 vim 支持不是很好（？）好像不支持捕获组（也有可能是我命令输的有问题） 于是只能换方法 后来把我的需求描述清楚，去问 qwen, 说可以用 obsidian 的 templater 插件，并且给了相应的代码， 然后我按照他给的代码，去做，发现不行。 然后根据我之前的经历，可能是模型不够强 于是我去用 cursor cursor 给了代码，能使用 不过一开始不符合我的需求 然后经过不断的调整，描述自己的需求，最终成功实现 感想 第一个就是，就像 jyy 说的，要是重复做什么事情三次以上，就该想想有没有现成的自动化工具，如果没有，就看看自己能不能写出来，这是学计算机的人很需要的一个技能 然后就是 ai 的使用吧，比较难度高的任务还是得用好点的 ai, 不然错误太多了。 ","date":"2025-10-05T00:00:00Z","permalink":"https://mcig-ggg.github.io/p/%E9%81%BF%E5%85%8D%E6%97%A0%E6%84%8F%E4%B9%89%E7%9A%84%E9%87%8D%E5%A4%8D%E8%AE%B0%E4%B8%80%E6%AC%A1obsidian%E6%8A%98%E8%85%BE/","title":"避免无意义的重复：记一次obsidian折腾"},{"content":"介绍 Hugo是一个用Go语言编写的静态网站生成器，以其快速、灵活和易用性而闻名。本文将介绍如何使用Hugo创建个人博客并将其部署到GitHub Pages上。\n准备工作 安装Hugo 1 2 3 4 5 # Ubuntu/Debian sudo apt-get install hugo # macOS brew install hugo 安装Git 1 2 # Ubuntu/Debian sudo apt-get install git 创建新的Hugo站点 创建新站点 1 2 hugo new site my-blog cd my-blog 添加主题（以Stack主题为例） 1 2 git init git submodule add https://github.com/CaiJimmy/hugo-theme-stack.git themes/hugo-theme-stack 配置站点 创建或编辑hugo.yaml文件，添加基本配置： 1 2 3 4 baseURL: \u0026#39;https://yourusername.github.io/\u0026#39; languageCode: \u0026#39;zh-cn\u0026#39; title: \u0026#39;我的博客\u0026#39; theme: \u0026#39;hugo-theme-stack\u0026#39; 创建新文章 使用以下命令创建新文章：\n1 hugo new post/my-first-post/index.md 设置博客封面图片 Hugo支持为每篇文章设置封面图片，有以下几种方式：\n在文章前言（Front Matter）中设置 1 2 3 4 5 --- title: \u0026#34;文章标题\u0026#34; date: 2025-06-01 image: cover.jpg # 图片放在文章同级目录下 --- 使用外部图片链接 1 2 3 4 5 --- title: \u0026#34;文章标题\u0026#34; date: 2025-06-01 image: \u0026#34;https://example.com/image.jpg\u0026#34; # 使用完整的URL --- 图片存放规则：\n将图片放在文章目录下（推荐）： 1 2 3 4 5 content/ └── post/ └── my-first-post/ ├── index.md └── cover.jpg 或放在全局静态目录： 1 2 3 static/ └── images/ └── cover.jpg 然后在文章中引用：image: \u0026quot;/images/cover.jpg\u0026quot; 图片最佳实践：\n建议使用 JPEG 或 WebP 格式 推荐尺寸：1200x630 像素（16:9） 控制图片大小在 500KB 以内 使用有意义的文件名，避免特殊字符 本地预览 运行Hugo服务器进行本地预览：\n1 hugo server -D 访问 http://localhost:1313 查看效果。\n部署到GitHub Pages 在部署之前，需要明确理解Hugo博客的两个重要概念：\n源码目录：包含所有的文章源文件、配置文件和主题 构建目录（public/）：包含Hugo生成的静态网站文件 为了正确管理博客，我们需要创建两个独立的GitHub仓库：\n博客源码仓库（比如：blog-source）\n用途：存储和版本控制博客的源代码 内容：所有的markdown文件、配置文件、主题等 特点：需要在 .gitignore 中忽略 public/ 目录，因为它是构建产物 静态网站仓库（必须命名为：username.github.io）\n用途：通过GitHub Pages提供网站访问 内容：只包含 public/ 目录中的构建产物 特点：这个仓库就是您的网站的实际托管位置 部署步骤 1. 设置源码仓库 首先，在博客根目录（blog/）下执行：\n1 2 3 4 5 6 7 8 9 10 # 1. 创建 .gitignore 文件，忽略构建产物 echo \u0026#34;public/\u0026#34; \u0026gt; .gitignore # 2. 初始化源码仓库 git init git add . git commit -m \u0026#34;Initial commit: Blog source code\u0026#34; git branch -M main git remote add origin https://github.com/username/blog-source.git git push -u origin main 2. 设置静态网站仓库 然后，处理构建后的静态文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 1. 生成静态网站文件 hugo --minify # 2. 进入构建目录 cd public # 3. 初始化静态网站仓库 git init git add . git commit -m \u0026#34;Initial commit: Static website files\u0026#34; git branch -M main git remote add origin https://github.com/username/username.github.io.git git push -u origin main 更新博客流程 每次更新博客内容时：\n在源码仓库中修改内容（比如写新文章）： 1 2 3 4 # 在blog目录下 git add . git commit -m \u0026#34;Add new blog post\u0026#34; git push 更新静态网站： 1 2 3 4 5 6 7 # 在blog目录下 hugo --minify # 重新生成静态文件 cd public # 进入构建目录 git add . # 添加新生成的文件 git commit -m \u0026#34;Update website\u0026#34; # 提交更改 git push # 推送到GitHub Pages cd .. # 返回博客根目录 这种双仓库的设置确保了：\n源代码得到完整的版本控制 构建产物独立管理 GitHub Pages能正确托管网站 提示：如果这个手动部署过程太繁琐，可以使用GitHub Actions自动化它。\n自动化部署 可以使用GitHub Actions实现自动部署。这种方式的优势是：\n无需手动构建和部署 每次推送代码后自动更新网站 确保部署过程的一致性 可以在任何设备上更新博客 在仓库根目录创建.github/workflows/deploy.yml文件，这个配置文件的主要功能是：\n触发条件： 1 2 3 4 5 on: push: branches: - main # 当推送到main分支时触发 workflow_dispatch: # 支持手动触发部署 权限设置： 1 2 3 4 permissions: contents: read # 仓库内容的读取权限 pages: write # GitHub Pages的写入权限 id-token: write # 身份令牌的写入权限 构建任务（build job）：\n检出代码：使用 actions/checkout@v3 获取仓库代码 设置Hugo环境：安装最新版本的Hugo 构建网站：运行 hugo --minify 生成优化后的静态文件 上传构建结果：将 public 目录打包作为部署制品 部署任务（deploy job）：\n等待构建任务完成 将构建好的文件部署到GitHub Pages 输出部署后的网站URL 完整的配置文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 name: Deploy Hugo site to Pages on: push: branches: - main workflow_dispatch: permissions: contents: read pages: write id-token: write concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: false defaults: run: shell: bash jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; - name: Build with Hugo run: hugo --minify - name: Upload artifact uses: actions/upload-pages-artifact@v2 with: path: ./public deploy: environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v2 要启用自动部署，还需要进行以下设置：\n在GitHub仓库设置中启用Pages功能：\n进入仓库的 Settings \u0026gt; Pages 在 Build and deployment 部分 Source 选择 \u0026ldquo;GitHub Actions\u0026rdquo; 确保仓库的 Actions 权限已开启：\n进入仓库的 Settings \u0026gt; Actions \u0026gt; General 确保 Actions permissions 允许工作流运行 设置完成后，每次你推送更改到main分支，GitHub Actions就会自动：\n拉取最新代码 安装Hugo环境 构建你的网站 部署到GitHub Pages 你可以在仓库的Actions标签页查看部署状态和日志。\n维护博客 定期更新内容 1 hugo new post/new-article/index.md 本地预览 1 hugo server -D 部署更新 1 2 3 4 hugo --minify git add . git commit -m \u0026#34;Update content\u0026#34; git push 结论 现在你已经掌握了使用Hugo创建博客并部署到GitHub Pages的基本流程。通过这个方法，你可以免费托管你的个人博客，并且可以方便地进行更新和维护。\n常见问题解答 配置评论系统 Stack主题支持多种评论系统，包括：\nDisqus（默认配置） DisqusJS Utterances（GitHub Issues驱动） Giscus（GitHub Discussions驱动） Twikoo Waline 等其他系统 配置Disqus（最简单） 在 Disqus官网 注册账号 创建一个站点，获取shortname 修改 hugo.yaml 中的配置： 1 2 3 4 5 6 7 8 services: disqus: shortname: \u0026#34;你的shortname\u0026#34; # 替换为你的Disqus shortname params: comments: enabled: true provider: disqus # 使用disqus作为评论系统 配置Utterances（基于GitHub） 安装 Utterances GitHub App 修改 hugo.yaml 配置： 1 2 3 4 5 6 7 8 params: comments: enabled: true provider: utterances utterances: repo: owner/repo-name # 替换为你的GitHub仓库 issueTerm: pathname # 文章和issue的映射方式 label: comment # issue的标签 配置Giscus（新一代GitHub评论） 安装 Giscus GitHub App 在仓库设置中启用Discussions功能 访问 Giscus.app 生成配置 修改 hugo.yaml 配置： 1 2 3 4 5 6 7 8 9 10 11 12 params: comments: enabled: true provider: giscus giscus: repo: owner/repo-name # 你的GitHub仓库 repoID: \u0026#34;R_xxx\u0026#34; # 仓库ID category: \u0026#34;Announcements\u0026#34; # discussion分类 categoryID: \u0026#34;DIC_xxx\u0026#34; # 分类ID mapping: \u0026#34;pathname\u0026#34; # 映射方式 reactionsEnabled: 1 # 允许反应 emitMetadata: 0 # 元数据 最佳实践 对于个人博客：\n如果想要最简单的设置：选择Disqus 如果想要与GitHub集成：选择Giscus 如果注重隐私：选择self-hosted的Waline 评论系统管理：\n定期审核评论 设置合适的垃圾评论过滤 保持与读者的互动 性能考虑：\n评论系统通常会增加页面加载时间 考虑使用懒加载方式加载评论系统 可以设置评论区在用户滚动到底部时才加载 ","date":"2025-06-01T00:00:00Z","image":"https://mcig-ggg.github.io/p/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8hugo%E5%88%9B%E5%BB%BA%E5%B9%B6%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/screenshot_hu_d11d3cabe86efe47.png","permalink":"https://mcig-ggg.github.io/p/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8hugo%E5%88%9B%E5%BB%BA%E5%B9%B6%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","title":"如何使用Hugo创建并部署个人博客"}]